const Chat = require("./model");
const OpenAI = require("openai");
const { GoogleGenerativeAI } = require("@google/generative-ai");
const { CohereClient } = require("cohere-ai");

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const gemini = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
const cohere = new CohereClient({token: process.env.COHERE_API_KEY});

const githubClient = new OpenAI({
  apiKey: process.env.GITHUB_TOKEN,
  baseURL: "https://models.github.ai/inference"
});

let intent = null;
let reply = "";
function detectIntent(message) {
  const msg = message.toLowerCase();

  if (
    msg.includes("code") ||
    msg.includes("algorithm") ||
    msg.includes("bug") ||
    msg.includes("error") ||
    msg.includes("debug")
  ) {
    return "code";
  }

  if (
    msg.includes("why") ||
    msg.includes("how") ||
    msg.includes("explain") ||
    msg.includes("proof")
  ) {
    return "reasoning";
  }

  if (
    msg.includes("summarize") ||
    msg.includes("summary") ||
    msg.includes("shorten")
  ) {
    return "summary";
  }

  return "factual";
}


const handleChat = async (req, res) => {
  const { message, model, chatId } = req.body;

  try {
    let chat;

    // Load or create chat
    if (chatId) {
      chat = await Chat.findById(chatId);
    } else {
      chat = await Chat.create({
        title: message.slice(0, 30),
        messages: []
      });
    }

    // Save user message
    chat.messages.push({ role: "user", content: message });

    // let reply = "";

    // Prepare full context
    const history = chat.messages.map(m => ({
      role: m.role,
      content: m.content
    }));

    // OpenAI
    if (model === "openai") {
      const completion = await openai.chat.completions.create({
        model: "gpt-4o-mini",
        messages: history
      });
      reply = completion.choices[0].message.content;
    }

    // Gemini (context preserved)
    if (model === "gemini") {
  try {
    const gModel = gemini.getGenerativeModel({
      model: "gemini-2.5-flash"
    });

    const prompt = history
      .map(m => `${m.role.toUpperCase()}: ${m.content}`)
      .join("\n\n");

    const result = await gModel.generateContent(prompt);
    reply = result.response.text();
  } catch (gErr) {
    console.error("GEMINI ERROR:", gErr);
    throw gErr;
  }
}

    if (model === "cohere") {
      try {
        const messages = history.map(m => ({
          role: m.role,
          content: m.content
        }));

        const completion = await githubClient.chat.completions.create({
          model: "cohere/Cohere-command-r-08-2024",
          messages,
          max_tokens: 300,
          temperature: 0.7
        });

        reply = completion.choices[0].message.content;
      } catch (err) {
        console.error("GITHUB COHERE ERROR:", err);
        throw err;
      }
    }

    // AUTO mode

    if (model === "auto") {
  const intent = detectIntent(message);
  console.log("AUTO MODE intent:", intent);

  try {
    // CODE & REASONING → GPT-4o
    if (intent === "code" || intent === "reasoning") {
      const completion = await openai.chat.completions.create({
        model: "gpt-4o-mini",
        messages: history
      });
      reply = completion.choices[0].message.content;
    }

    // SUMMARY → COHERE (GitHub Models)
    else if (intent === "summary") {
      const completion = await githubClient.chat.completions.create({
        model: "cohere/Cohere-command-r-08-2024",
        messages: history,
        max_tokens: 300
      });
      reply = completion.choices[0].message.content;
    }

    // FACTUAL / SEARCH → GEMINI
    else {
      const gModel = gemini.getGenerativeModel({
        model: "gemini-2.5-flash"
      });

      const prompt = history
        .map(m => `${m.role.toUpperCase()}: ${m.content}`)
        .join("\n\n");

      const result = await gModel.generateContent(prompt);
      reply = result.response.text();
    }
  } catch (autoErr) {
    console.error("AUTO MODE FAILED — FALLBACK TO GPT-4o", autoErr);

    // FINAL fallback (never fail)
    const completion = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: history
    });
    reply = completion.choices[0].message.content;
  }
}

    if (!reply || reply.trim() === "") {
  throw new Error("Reply was not generated by any model");
}

    // Save assistant response
    chat.messages.push({
  role: "assistant",
  content: reply,
  model: model === "auto" ? intent : model
});


    await chat.save();

    res.json({ reply, chatId: chat._id });

  } catch (err) {
    res.status(500).json({ error: err.message });
  }
};

module.exports = { handleChat };
